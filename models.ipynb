{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# firstly the fasttext model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import fasttext\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from emoji import demojize\n",
    "import re\n",
    "from sklearn.metrics import classification_report"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# preprocess hate data same way as abusive data and\n",
    "# replace \\n with space\n",
    "with open('data/test_hate.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    # replace \\n with space\n",
    "    for i in range(len(data)):\n",
    "        data[i][0] = re.sub(r'https.*[^ ]', 'URL', data[i][0])\n",
    "        data[i][0] = re.sub(r'http.*[^ ]', 'URL', data[i][0])\n",
    "        data[i][0] = re.sub(r'@([^ ]*)', '@USER', data[i][0])\n",
    "        data[i][0] = demojize(data[i][0])\n",
    "        data[i][0] = re.sub(r'(:.*?:)', r' \\1 ', data[i][0])\n",
    "        data[i][0] = re.sub(' +', ' ', data[i][0])\n",
    "        data[i][0] = data[i][0].replace('\\n', ' ')\n",
    "    # write to csv\n",
    "    with open('data/test_hate.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# divide test set in 303 hate tweets and 697 non-hate tweets\n",
    "# Caution: Creates new different test set, for consistency use\n",
    "# test_hate_sorted_definitive.csv\n",
    "\n",
    "with open('data/test_hate.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "    hate_data = []\n",
    "    non_hate_data = []\n",
    "\n",
    "    # first get al the hate tweets\n",
    "    # yes this way is slower but it is easier to understand\n",
    "    for row in data:\n",
    "        if row[1] == 'y':\n",
    "            hate_data.append(row)\n",
    "    # now that we got the hate tweets, we can select at random the non-hate tweets\n",
    "    for row in data:\n",
    "        if row[1] == 'n':\n",
    "            non_hate_data.append(row)\n",
    "\n",
    "    # now create data set with all hate tweets and 697 non-hate tweets randomly selected\n",
    "    # we will use this data set to test the model\n",
    "    test_data = []\n",
    "    for i in range(len(hate_data)):\n",
    "        test_data.append(hate_data[i])\n",
    "\n",
    "    # get random indexes\n",
    "    random_indexes = random.sample(range(len(non_hate_data)), 697)\n",
    "    # add the non-hate tweets to the test data\n",
    "    for i in random_indexes:\n",
    "        test_data.append(non_hate_data[i])\n",
    "    # write to csv\n",
    "    with open('data/test_hate_sorted.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(test_data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def convert_to_fastext_format(path, delimiter):\n",
    "    # convert csv to correct fasttext format\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=delimiter)\n",
    "        data = list(reader)\n",
    "        # for every row but the first move the last column to beginning and add text '__label__'\n",
    "        # and change both 'IMPLICIT' and 'EXPLICIT' to 'ABUSIVE'\n",
    "        for i in range(1, len(data)):\n",
    "            if data[i][1] == 'IMPLICIT':\n",
    "                data[i][1] = 'ABUSIVE'\n",
    "            elif data[i][1] == 'EXPLICIT':\n",
    "                data[i][1] = 'ABUSIVE'\n",
    "\n",
    "            # if prossesing the hate data\n",
    "            if 'hate' in path:\n",
    "                if data[i][1] == 'y':\n",
    "                    data[i][1] = 'ABUSIVE'\n",
    "                elif data[i][1] == 'n':\n",
    "                    data[i][1] = 'NOT'\n",
    "            data[i][0] = '__label__' + data[i][1] + ' ' + data[i][0]\n",
    "            # remove the last column\n",
    "            data[i] = data[i][:-1]\n",
    "\n",
    "        if 'test' in path:\n",
    "            new_path = path.replace('test', 'test_fasttext')\n",
    "        if 'train' in path:\n",
    "            new_path = path.replace('train', 'train_fasttext')\n",
    "        if 'dev' in path:\n",
    "            new_path = path.replace('dev', 'dev_fasttext')\n",
    "\n",
    "        new_path = new_path.replace('.csv', '.txt')\n",
    "\n",
    "        # write to file excluding the first row\n",
    "        with open(new_path, 'w') as f:\n",
    "            for row in data[1:]:\n",
    "                f.write('\\t'.join(row) + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "convert_to_fastext_format('data/train_abusive.csv', '\\t')\n",
    "convert_to_fastext_format('data/test_abusive.csv', '\\t')\n",
    "convert_to_fastext_format('data/dev_abusive.csv', '\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "convert_to_fastext_format('data/test_hate_sorted_definitive.csv', ',')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## train fasttext model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# tune fasttext parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:   90 Best score:  0.644444 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  23825\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  144794 lr:  0.000000 avg.loss:  0.167068 ETA:   0h 0m 0s\rwords/sec/thread:  146423 lr:  0.046046 avg.loss:  0.396957 ETA:   0h 0m 6sProgress:  64.9% words/sec/thread:  148905 lr:  0.020642 avg.loss:  0.216720 ETA:   0h 0m 2s\n"
     ]
    }
   ],
   "source": [
    "# autotune the model\n",
    "\n",
    "model = fasttext.train_supervised('data/train_fasttext_abusive.txt', autotuneValidationFile='data/dev_fasttext_abusive.txt', autotuneDuration=300, autotuneMetric=\"f1:__label__ABUSIVE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# model = fasttext.train_supervised('data/train_fasttext_abusive.txt', epoch=25)\n",
    "# save model\n",
    "model.save_model('model/fasttext_model_300sec_FINAL.bin')\n",
    "\n",
    "# load model\n",
    "# model = fasttext.load_model('model/fasttext_model_bew.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(('__label__NOT', '__label__ABUSIVE'), array([0.94600821, 0.05401177]))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"@USER @USER @USER @USER @USER Dit is pure provocatie door die gehersenspoelde moslim.\", k=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(1901, 0.7348763808521831, 0.7348763808521831)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on test_abusive_fasttext.txt\n",
    "model.test('data/test_fasttext_abusive.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(999, 0.7197197197197197, 0.7197197197197197)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on test_hate_fasttext.txt\n",
    "model.test('data/test_fasttext_hate_sorted.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "with open ('data/test_hate_sorted_definitive.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data_hate = list(reader)\n",
    "    # get predictions\n",
    "    for i, row in enumerate(data_hate):\n",
    "        row.append(model.predict(row[0], k=1)[0][0])\n",
    "        if \"NOT\" in row[2]:\n",
    "            data_hate[i][2] = \"n\"\n",
    "        elif \"ABUSIVE\" in row[2]:\n",
    "            data_hate[i][2] = \"y\"\n",
    "    # write changes to file\n",
    "    with open('data/test_hate_predictions.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data_hate)\n",
    "\n",
    "    # and save to var\n",
    "    predictions_hate = data_hate\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# get predictions for test_abusive (and save in abusive predictions)\n",
    "\n",
    "with open('data/test_abusive.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    data_abuse = list(reader)\n",
    "    # get predictions\n",
    "    for i, row in enumerate(data_abuse[1:]):\n",
    "        row.append(model.predict(row[0].replace('\\n', ' '), k=1)[0][0])\n",
    "        if \"NOT\" in row[2]:\n",
    "            data_abuse[i+1][2] = \"n\"\n",
    "        elif \"ABUSIVE\" in row[2]:\n",
    "            data_abuse[i+1][2] = \"y\"\n",
    "        if \"NOT\" in row[1]:\n",
    "            data_abuse[i+1][1] = \"n\"\n",
    "        elif \"EXPLICIT\" in row[1] or \"IMPLICIT\" in row[1]:\n",
    "            data_abuse[i+1][1] = \"y\"\n",
    "    # write changes to file\n",
    "    with open('data/test_abusive_predictions.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(data_abuse[1:])\n",
    "\n",
    "    # and save to var\n",
    "    predictions_abuse = data_abuse[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "y_true_hate = []\n",
    "y_pred_hate = []\n",
    "for row in predictions_hate:\n",
    "    y_true_hate.append(row[1])\n",
    "    y_pred_hate.append(row[2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "y_true_abuse = []\n",
    "y_pred_abuse = []\n",
    "for row in predictions_abuse:\n",
    "    y_true_abuse.append(row[1])\n",
    "    y_pred_abuse.append(row[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n      0.752     0.890     0.815       697\n",
      "           y      0.562     0.327     0.413       303\n",
      "\n",
      "    accuracy                          0.719      1000\n",
      "   macro avg      0.657     0.608     0.614      1000\n",
      "weighted avg      0.695     0.719     0.693      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create classification report for hate\n",
    "\n",
    "\n",
    "print(classification_report(y_true_hate, y_pred_hate, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           n      0.730     0.958     0.829      1264\n",
      "           y      0.781     0.297     0.430       637\n",
      "\n",
      "    accuracy                          0.736      1901\n",
      "   macro avg      0.755     0.627     0.629      1901\n",
      "weighted avg      0.747     0.736     0.695      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create classification report for abuse\n",
    "print(classification_report(y_true_abuse, y_pred_abuse, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# show misclassified tweets hate\n",
    "misclassified_hate = []\n",
    "for row in predictions_hate:\n",
    "    if row[1] != row[2]:\n",
    "        misclassified_hate.append([row[0], row[1], row[2]])\n",
    "\n",
    "import json\n",
    "# write to json\n",
    "json.dump(misclassified_hate, open('data/misclassified_hate.json', 'w'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moslim abuse: 15\n",
      "moslim not: 7\n"
     ]
    }
   ],
   "source": [
    "# count word 'moslim' in train data per class\n",
    "# note, current output is not from 'moslim'\n",
    "moslim_abuse = 0\n",
    "moslim_not = 0\n",
    "with open('data/train_abusive.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    data_abuse = list(reader)\n",
    "    for row in data_abuse[1:]:\n",
    "        if \"moslim\" in row[0].lower():\n",
    "            if \"NOT\" in row[1]:\n",
    "                moslim_not += 1\n",
    "            else:\n",
    "                moslim_abuse += 1\n",
    "\n",
    "print(\"moslim abuse:\", moslim_abuse)\n",
    "print(\"moslim not:\", moslim_not)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# load train data\n",
    "with open('data/train_abusive.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    data = list(reader)\n",
    "    text_only = []\n",
    "    labels = []\n",
    "    for row in data[1:]:\n",
    "        text_only.append(row[0])\n",
    "        if 'EXPLICIT' in row[1] or 'IMPLICIT' in row[1]:\n",
    "            labels.append('ABUSIVE')\n",
    "        else:\n",
    "            labels.append('NOT')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# load abusive test data in same way above\n",
    "with open('data/test_abusive.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    data = list(reader)\n",
    "    test_text_only_abuse = []\n",
    "    test_labels_abuse = []\n",
    "    for row in data[1:]:\n",
    "        test_text_only_abuse.append(row[0])\n",
    "        if 'EXPLICIT' in row[1] or 'IMPLICIT' in row[1]:\n",
    "            test_labels_abuse.append('ABUSIVE')\n",
    "        else:\n",
    "            test_labels_abuse.append('NOT')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# load hate test data in same way above\n",
    "with open('data/test_hate_sorted_definitive.csv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    data = list(reader)\n",
    "    test_text_only_hate = []\n",
    "    test_labels_hate = []\n",
    "    for row in data[1:]:\n",
    "        test_text_only_hate.append(row[0])\n",
    "        if 'y' in row[1]:\n",
    "            test_labels_hate.append('ABUSIVE')\n",
    "        else:\n",
    "            test_labels_hate.append('NOT')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# first vecotrize the data with tf-idf word 2-grams and character 5-grams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# create pipeline with 2-grams (Word) and 5-grams (Char)\n",
    "pipe = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('word', TfidfVectorizer(ngram_range=(2, 2), analyzer='word')),\n",
    "        ('char', TfidfVectorizer(ngram_range=(3, 5), analyzer='char')),\n",
    "    ])),\n",
    "    ('svm', SVC(kernel='linear', C=1.0)),\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('union',\n                 FeatureUnion(transformer_list=[('word',\n                                                 TfidfVectorizer(ngram_range=(2,\n                                                                              2))),\n                                                ('char',\n                                                 TfidfVectorizer(analyzer='char',\n                                                                 ngram_range=(3,\n                                                                              5)))])),\n                ('svm', SVC(kernel='linear'))])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "              'svm__kernel': ['linear'],\n",
    "              }\n",
    "\n",
    "# grid_search = GridSearchCV(pipe, param_grid=parameters) # best param is C=1\n",
    "# grid_search.fit(text_only, labels)\n",
    "\n",
    "#fit pipe to data (grid search resulted in C=1)\n",
    "pipe.fit(text_only, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ABUSIVE      0.595     0.291     0.391       302\n",
      "         NOT      0.749     0.914     0.823       697\n",
      "\n",
      "    accuracy                          0.726       999\n",
      "   macro avg      0.672     0.603     0.607       999\n",
      "weighted avg      0.702     0.726     0.692       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get hate report\n",
    "y_true_hate, y_pred_hate = test_labels_hate, pipe.predict(test_text_only_hate)\n",
    "print(classification_report(y_true_hate, y_pred_hate, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ABUSIVE      0.848     0.272     0.411       637\n",
      "         NOT      0.727     0.975     0.833      1264\n",
      "\n",
      "    accuracy                          0.740      1901\n",
      "   macro avg      0.787     0.624     0.622      1901\n",
      "weighted avg      0.767     0.740     0.692      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get abuse report\n",
    "y_true_abuse, y_pred_abuse = test_labels_abuse, pipe.predict(test_text_only_abuse)\n",
    "print(classification_report(y_true_abuse, y_pred_abuse, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## most frequent dummy classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "DummyClassifier(strategy='most_frequent')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import dummy\n",
    "\n",
    "dummy_clf_hate = dummy.DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf_abuse = dummy.DummyClassifier(strategy='most_frequent')\n",
    "dummy_clf_abuse.fit(test_text_only_abuse, test_labels_abuse)\n",
    "dummy_clf_hate.fit(test_text_only_hate, test_labels_hate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ABUSIVE      0.000     0.000     0.000       302\n",
      "         NOT      0.698     1.000     0.822       697\n",
      "\n",
      "    accuracy                          0.698       999\n",
      "   macro avg      0.349     0.500     0.411       999\n",
      "weighted avg      0.487     0.698     0.573       999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get hate report\n",
    "y_true_hate, y_pred_hate = test_labels_hate, dummy_clf_hate.predict(test_text_only_hate)\n",
    "print(classification_report(y_true_hate, y_pred_hate, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     ABUSIVE      0.000     0.000     0.000       637\n",
      "         NOT      0.665     1.000     0.799      1264\n",
      "\n",
      "    accuracy                          0.665      1901\n",
      "   macro avg      0.332     0.500     0.399      1901\n",
      "weighted avg      0.442     0.665     0.531      1901\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/matisse/PycharmProjects/scriptie/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get abuse report\n",
    "y_true_abuse, y_pred_abuse = test_labels_abuse, dummy_clf_abuse.predict(test_text_only_abuse)\n",
    "print(classification_report(y_true_abuse, y_pred_abuse, digits=3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}